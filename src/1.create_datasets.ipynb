{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. 분석을 위한 Dataset 생성\n",
    "* 목적\n",
    "    * 이후 진행될 모델링 및 분석 과정을 원활하게 수행하기 위한 Dataset을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% Package import\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Function Define\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data_keep_binary_label_ratio(X:pd.DataFrame, y: pd.Series, split_ratio: float = 0.7)\\\n",
    "    -> (pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    '''\n",
    "    '불량'의 비율을 유지하며 dataset을 분리한다.\n",
    "    '''\n",
    "    Y_index = y[y==0].index\n",
    "    N_index = y[y==1].index\n",
    "\n",
    "    Y_train_cnt = int(len(Y_index) * split_ratio)\n",
    "    N_train_cnt = int(len(N_index) * split_ratio)\n",
    "\n",
    "    Y_train_idx = np.random.choice(Y_index, Y_train_cnt, replace=False)\n",
    "    Y_test_idx = np.setdiff1d(Y_index, Y_train_idx)\n",
    "\n",
    "    N_train_idx = np.random.choice(N_index, N_train_cnt, replace=False)\n",
    "    N_test_idx = np.setdiff1d(N_index, N_train_idx)\n",
    "\n",
    "    train_idx = np.random.permutation(np.union1d(Y_train_idx, N_train_idx))\n",
    "    test_idx = np.random.permutation(np.union1d(Y_test_idx, N_test_idx))\n",
    "\n",
    "    X_train = X.loc[train_idx, :]\n",
    "    y_train = y[train_idx]\n",
    "    X_test = X.loc[test_idx, :]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Load Data\n"
    }
   },
   "outputs": [],
   "source": [
    "labeled_dir = './../dataset/original/labeled.csv'\n",
    "labeled_data = pd.read_csv(labeled_dir)\n",
    "labeled_data = labeled_data.drop_duplicates()\n",
    "\n",
    "unlabeled_dir = './../dataset/original/unlabeled.csv'\n",
    "unlabeled_data = pd.read_csv(unlabeled_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleansing\n",
    "- Labeled data & Unlabeled data의 EDA에서 확인된 동일한 값(0)을 가지고 있는 변수 제거\n",
    "- Y column (PassOrFail) 값 숫자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% 기본 전처리\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove columns\n",
    "drop_cols = ['Mold_Temperature_1', 'Mold_Temperature_2','Mold_Temperature_5', 'Mold_Temperature_6', 'Mold_Temperature_7',\n",
    "             'Mold_Temperature_8','Mold_Temperature_9', 'Mold_Temperature_10', 'Mold_Temperature_11', 'Mold_Temperature_12']\n",
    "labeled_data = labeled_data.drop(drop_cols, axis=1)\n",
    "unlabeled_data = unlabeled_data.drop(['Unnamed: 0']+drop_cols, axis=1)\n",
    "\n",
    "# Y 처리\n",
    "labeled_data['PassOrFail'] = labeled_data['PassOrFail'].map({'Y':0, 'N':1})\n",
    "\n",
    "# 저장\n",
    "labeled_data.to_csv('./dataset/labeled_new.csv', index=False)\n",
    "unlabeled_data.to_csv('./dataset/unlabeled_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split Dataset\n",
    "1. X, y 데이터 분리\n",
    "2. label 0, 1별로 index 분리\n",
    "3. 각 라벨에서 일정한 비율로 train, test index 랜덤하게 추출\n",
    "4. label0 train index + label1 train index -> train X, y data -> 같은 방식으로 train / validation 분리\n",
    "5. label0 test index + label1 test index -> test X, ydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Split data\n"
    }
   },
   "outputs": [],
   "source": [
    "labeled_X = labeled_data.iloc[:, 9:]\n",
    "labeled_y = labeled_data.PassOrFail\n",
    "\n",
    "unlabeled_X = unlabeled_data.iloc[:, 9:]\n",
    "\n",
    "labeled_X_train, labeled_y_train, labeled_X_test, labeled_y_test = split_data_keep_binary_label_ratio(labeled_X, labeled_y)\n",
    "labeled_X_train, labeled_y_train, labeled_X_valid, labeled_y_valid = split_data_keep_binary_label_ratio(labeled_X_train,\n",
    "                                                                                                        labeled_y_train)\n",
    "# train : 3916 / valid :1680 / test : 2400\n",
    "\n",
    "# save created data\n",
    "def save_created_dataset(data, file_name):\n",
    "    save_dir = './dataset'\n",
    "    data.to_csv(os.path.join(save_dir, file_name), index=False)\n",
    "\n",
    "save_created_dataset(labeled_X_train, 'labeled_X_train.csv')\n",
    "save_created_dataset(labeled_y_train, 'labeled_y_train.csv')\n",
    "\n",
    "save_created_dataset(labeled_X_valid, 'labeled_X_valid.csv')\n",
    "save_created_dataset(labeled_y_valid, 'labeled_y_valid.csv')\n",
    "\n",
    "save_created_dataset(labeled_X_test, 'labeled_X_test.csv')\n",
    "save_created_dataset(labeled_y_test, 'labeled_y_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% Scaling\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "          ('scaler', MinMaxScaler())])\n",
    "\n",
    "pipeline.fit(unlabeled_X)\n",
    "\n",
    "unlabeled_X_scaled = pd.DataFrame(pipeline.transform(unlabeled_X), columns=unlabeled_X.columns)\n",
    "labeled_X_train_scaled = pd.DataFrame(pipeline.transform(labeled_X_train), columns=labeled_X_train.columns)\n",
    "labeled_X_valid_scaled = pd.DataFrame(pipeline.transform(labeled_X_valid), columns=labeled_X_valid.columns)\n",
    "labeled_X_test_scaled = pd.DataFrame(pipeline.transform(labeled_X_test), columns=labeled_X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_created_dataset(unlabeled_X_scaled, 'unlabeled_X_scaled.csv')\n",
    "save_created_dataset(labeled_X_train_scaled, 'labeled_X_train_scaled.csv')\n",
    "save_created_dataset(labeled_X_valid_scaled, 'labeled_X_valid_scaled.csv')\n",
    "save_created_dataset(labeled_X_test_scaled, 'labeled_X_test_scaled.csv')\n",
    "\n",
    "with open('./dataset/scaling_model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
